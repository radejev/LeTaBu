---
title: "Block 8: Metadaten: Standards und Schnittstellen"
date: 2023-05-09
---

Im Block vom 9. Mai  haben wir über MARC, DublinCore, OpenRefine und OAI-PMH gesprochen. Man könnte fast sagen, dass nun alle Fäden zusammenlaufen. Ich habe die Eindrücke aus dem Block zusammengefasst und mit Erkenntnissen aus einer Internetrecherche ergänzt: eine Full-Circle-Experience!

 
## Recap: XML und Metadaten

Die Auszeichnungssprache XML (Extensible Markup Language) wird in vielen Arbeitsbereichen verwendet, um deskriptive Daten und Informationen zu Dokumenten, Objekten und Produkten zu erfassen und zwischen verschiedenen Akteuren zu teilen. In Bibliotheken werden XML-basierte Metadatenstandards wie MARC (Machine-Readable Cataloguing) und Dublin Core verwendet. MARC ist dabei etwas stärker auf den Bibliotheksbetrieb zugeschnitten, während DublinCore auch im Kontext von Museen oder Archiven verbreitet ist. In Archiven gilt Encoded Archival Description (EAD) als gängig.  Auf MARC/DublinCore bin ich bereits im  *[Blogpost Nr. 4](https://radejev.github.io/LeTaBu/2023/03/07/block4.html)* eingegangen. 

## Recap: Open-Refine


Im Zusammenhang mit Metadata-Operationen haben wir mehrfach über OpenRefine geredet (siehe Selbsttest *[Blogpost Nr. 3](https://radejev.github.io/LeTaBu/2023/02/28/block3.html)*). Zu den Herausforderungen in diesem Kontext gehören bekannterweise uneinheitliche Datenerfassung(-standards), mögliche Unvollständigkeit und Inkonsistenz der Daten sowie die benötigte Rechenleistung bei grossen Datenmengen. OpenRefine gilt als mächtiges Open-Source-Tool das mit Plug-Ins erweitert und an die spezifischen Situationsbedürfnisse angepasst werden kann. Für gewisse Vorgänge kann es von Vorteil sein, wenn Grundkenntnisse in Datenbank- oder Programmiersprachen vorhanden sind [^1].

## Recap: Harvesting

Sogenanntes «Harvesting» beschreibt das Abrufen von Metadatensätzen systemfremder Repositorien. Das dazu verwendete OAI-PMH-Protokoll definiert den Standard für Anfragen via HTTP. Wenn Anwendungen über OAI-PMH Metadaten herunterladen, können auch Kriterien als Filter gesetzt werden, damit nur relevante Datensätze in der HTTP-Antwort enthalten sind [^2]. Das «Harvesten» von Metadaten über OAI-PMH hat sich wegen der Einfachheit, Effizienz und technischen Zugänglichkeit zu einer beliebten Vorgehensweise für den Metadaten-Austausch in wissenschaftlichen, bibliothekarischen und archivarischen Kontexten entwickelt.

## Recap: ETL als Prozess

Als ETL-Prozess bezeichnet man die Migration von Daten zwischen unterschiedlichen Datensystemen, beispielsweise bei der Ablösung von Legacy-Systemen. Das Akronym ETL beschreibt dabei die drei Hauptphasen des Arbeitsprozesses:  Extraktion (Extraction), Transformation und «Herein»-Laden (Loading). Die Extraktion beschreibt das Exportieren der Daten aus dem Quellsystem. Als Transformation bezeichnet man die Konversion in ein fürs Zielsystem geeignetes Format sowie generelle Datenvereinheitlichung- und -bereinigung mit dem Ziel Duplikate und Fehlern vor dem Import ins Nachfolgesystem zu erkennen und beheben. Das Loading der Daten stellt den erwähnten Import dar. Dies beinhaltet das Datenmapping zwischen Quell- und Zielsystem sowie automatisiertes Logging von Unregelmässigkeiten, die in der Nachbearbeitung behoben werden sollen [^3] [^4].

# Links

[^1]:[Openschoolmaps.ch: Lehrmittel. ](https://openschoolmaps.ch/lehrmittel/data_cleansing_integration/arbeitsblaetter/data_cleansing_und_itegration_openrefine.html) 
[^2]:[OpenArchives: OAI Specification. ]( http://www.openarchives.org/OAI/openarchivesprotocol.html)
[^3]:[Bigdata-Insider: Was ist ETL? ](https://www.bigdata-insider.de/was-ist-etl-extract-transform-load-a-776549/)
[^4]:[Towards Data Science: Beginners Guide to ETL. ](https://towardsdatascience.com/beginners-guide-extract-transform-load-etl-49104a8f9294)